# Data Extraction For New Profiles


We know that Firefox Release 56 was released on August 8, 2017 and Firefox Beta 57
was released on September 25, 2017.

What is a random sample of **new release 56 users**? Anybody who downloaded version 56
between August 8, 2017 and September 24, 2017. The days they were on release 56 would
correspond to their release 56 usage. Similarly for 57. We restrict out download
days to the time when 56 was the most recent version available as opposed to
including new users of 56 when 57 was available.  We do something similar for 55 too.

However to get results out in a timely manner, we'll focus on new profiles
created in first two weeks of release of the corresponding beta and three weeks
of their usage data (from the time of their created profile).



```{python eval=FALSE}
import sys
import datetime
import random
import subprocess
from pyspark.sql import SparkSession
spark = SparkSession.builder.getOrCreate()
ms = spark.read.option("mergeSchema", "true").\
     parquet("s3://telemetry-parquet/main_summary/v4/")
ms.createOrReplaceTempView('ms')

import mozillametricstools.common.functions as mozfun
mozfun.register_udf(sqlContext
                    , lambda arr:  sum(arr) if arr else 0, "array_sum"
                    , pyspark.sql.types.IntegerType())

def newprofiles(version,minpcd,maxpcd,minsubmission,maxsubmission,minsubsession,maxsubsession):
    st= """
        SELECT
        client_id as cid,
        'new' as persona,
        profile_creation_date as pcd,
        datediff(
        from_unixtime(unix_timestamp(substr(subsession_start_date,1,10), 'yyyy-MM-dd')),
        date_add('1970-01-01', profile_creation_date)
        ) AS tSincePCreate,
        '{version}' as cohort,
        substr(app_version,1,2) as version,
        sum(subsession_length / 60.0 / 60) AS sh,
        sum(active_ticks * 5 /3600.0) as ah,
        sum(coalesce(scalar_parent_browser_engagement_unique_domains_count,0))  as domainsvisited,
        sum(coalesce(scalar_parent_browser_engagement_total_uri_count,0))  AS uris,
        sum(coalesce(scalar_parent_browser_engagement_tab_open_event_count,0)) as tabsopened,
        sum(coalesce(crash_submit_success_main,0)) as cm,
        sum(coalesce(crashes_detected_content, 0))- sum(coalesce(shutdown_kill,0)) as cc,
        sum(coalesce(crashes_detected_plugin,0))+sum(coalesce(crashes_detected_gmplugin,0)) as cp,
        sum(case
         when search_counts is not null then array_sum(search_counts.count) else 0
        end)  as search
        FROM ms
        WHERE
        normalized_channel = 'release'
        and app_name = 'Firefox'
        and profile_creation_date >= {minpcd} and profile_creation_date<={maxpcd} 
        and substr(subsession_start_date,1,10) >= '{minsubsession}' and substr(subsession_start_date,1,10)<='{maxsubsession}'
        and submission_date_s3 >= '{minsubmission}' and submission_date_s3 <= '{maxsubmission}'
        and sample_id >='11' and sample_id <='20'
        group by 1,2,3,4,5,6
        """.format(version=version,
                   minpcd=minpcd,
                   maxpcd=maxpcd, 
                   minsubmission=minsubmission,
                   maxsubmission=maxsubmission,
                   minsubsession=minsubsession,
                   maxsubsession=maxsubsession
        )
    newprofiles = spark.sql(st)
    newprofiles.createOrReplaceTempView("np")
    x2 = spark.sql("""
         with a as (select cid,min(version) as VersionCreatedOn from np group by 1 having VersionCreatedOn='{version}')
         select np.* from np join a on np.cid=a.cid where np.version='{version}' and tSincePCreate>=0
     """.format(version=version))
    x2.createOrReplaceTempView("x2")
    f = spark.sql(""" select *,
case when cm+cc+cp >0  then 1 else 0 end as crq,
    1 as dayPresent,
case when sh>0 and uris>0 then 1 else 0 end as active,
    cm+cc as tc,
case when  tSincePCreate  <=6 then 1 
            when  tSincePCreate  <=13 then 2 
            when  tSincePCreate <=27 then 3
            when  tSincePCreate <=34 then 4
            else 5 end as week
from x2 where pcd>0 and sh>=0""")
    return [st,f]


```

On a slight digression, it is possible for a user to download Firefox, to create
a new profile and yet show a `submission_start_date` later than their profile
creation date. For example, the output of the following query is in [https://sql.telemetry.mozilla.org/queries/49679/source](redash)

```{sql eval=FALSE}
 select * from main_summary
 WHERE      client_id='0e1715b9-53c3-43a3-bd95-235226a2b51d'
            and normalized_channel = 'release'
            and app_name = 'Firefox'
            and profile_creation_date =17387
            and substr(subsession_start_date,1,10) >= '2017-08-09' and substr(subsession_start_date,1,10)<='2017-09-12'
            and submission_date_s3 >= '20170809' and submission_date_s3 <= '20170821'
            and sample_id ='19' order by submission_date_s3
```

```{python eval=FALSE}
## Get dates for 56
releases = { 
    '55': datetime.datetime.strptime("2017-08-08","%Y-%m-%d").date(),
    '56': datetime.datetime.strptime("2017-09-25","%Y-%m-%d").date(),
    '57': datetime.datetime.strptime("2017-11-13","%Y-%m-%d").date(),
    'origin': datetime.datetime.strptime("1970-01-01","%Y-%m-%d").date()
}

## 21+14 = for profiles created on the 14th day ...
## extra days for submission date to handle slow returns from client
LAGBUFF=10
ACQPERIOD=7
DURA=3*7

cohort55 = newprofiles('55', 
                       minpcd=(releases['55']-releases['origin']).days,
                       maxpcd=(releases['55']-releases['origin']).days+ACQPERIOD,
                       minsubmission=releases['55'].strftime("%Y%m%d"),
                       maxsubmission=(releases['55']+datetime.timedelta(days=DURA+ACQPERIOD+LAGBUFF)).strftime("%Y%m%d"),
                       minsubsession=releases['55'].strftime("%Y-%m-%d"),
                       maxsubsession=(releases['55']+datetime.timedelta(days=DURA+ACQPERIOD)).strftime("%Y-%m-%d"))
                       

cohort56 = newprofiles('56', 
                       minpcd=(releases['56']-releases['origin']).days,
                       maxpcd=(releases['56']-releases['origin']).days+ACQPERIOD,
                       minsubmission=releases['56'].strftime("%Y%m%d"),
                       maxsubmission=(releases['56']+datetime.timedelta(days=DURA+ACQPERIOD+LAGBUFF)).strftime("%Y%m%d"),
                       minsubsession=releases['56'].strftime("%Y-%m-%d"),
                       maxsubsession=(releases['56']+datetime.timedelta(days=DURA+ACQPERIOD)).strftime("%Y-%m-%d"))
                       

cohort57 = newprofiles('57', 
                       minpcd=(releases['57']-releases['origin']).days,
                       maxpcd=(releases['57']-releases['origin']).days+ACQPERIOD,
                       minsubmission=releases['57'].strftime("%Y%m%d"),
                       maxsubmission=(releases['57']+datetime.timedelta(days=DURA+ACQPERIOD+LAGBUFF)).strftime("%Y%m%d"),
                       minsubsession=releases['57'].strftime("%Y-%m-%d"),
                       maxsubsession=(releases['57']+datetime.timedelta(days=DURA+ACQPERIOD)).strftime("%Y-%m-%d"))


cohorts = cohort57[1].union(cohort56[1].union(cohort55[1]))
cohorts = cohorts.cache()
cohorts.createOrReplaceTempView("cohorts")

## Saving the data
import subprocess
O = "s3://mozilla-metrics/user/sguha/tmp/betanew"
subprocess.call(["aws", "s3", "rm", "--recursive", O])
write = pyspark.sql.DataFrameWriter(cohorts)
write.parquet(path=O ,mode='overwrite')

cohorts  = spark.read.option("mergeSchema", "true").parquet(O)
cohorts.createOrReplaceTempView("cohorts")
cohorts = cohorts.cache()

```

**Create record** of new profiles in each cohort

```{python eval=FALSE}
newprofilesByCohort = spark.sql(
    """ select cohort, min(pcd) as minpcd, max(pcd) as maxpcd,
    count(distinct(cid)) as n 
    from cohorts 
    group by cohort 
    order by 1""")
```


**Create a database** of client_id, cohort ...
and whether profile was active in week 1 or week 2 (not cumulative). This and
(1) are enough for the retention bit
 
```{python eval=FALSE}
activeProfiles = spark.sql(""" select cid, cohort,
 week, case when sum(active)>0 then 1 else 0 end as activeIn ,
case when sum(dayPresent)>0 then 1 else 0 end as activeBasic
from cohorts group
 by 1,2,3 """)
```

and one for crashers

```{python eval=FALSE}
crashprofiles = spark.sql(""" select cid, cohort,
 week, case when sum(crq)>0 then 1 else 0 end as crashedIn  from cohorts group
 by 1,2,3 """)
```

**For measures such as **  session length, active session length, ...
intensity, domains, uris, tabsopenend, we need to look at the average value per
profile cumulative till week i. However,

- filter out rows for which measure is greater than the 99.9% (for that measure)
- add `C = 1/60` to the denominator for things like domain/per hour/profile (since we often get NaN)

```{python eval=FALSE}
pcut = [0.999]
def cut(v):
    return cohorts.approxQuantile(v,pcut,0.001)

C = 1/60.0
shCut = cut('sh')
ahCut =cut('ah')
searchCut = cut('search')
domCut = cut('domainsvisited')
uriCut = cut('uris')
tabCut = cut('tabsopened')

shdf = spark.sql("""
with a as 
(select cid, cohort, week, count(distinct(tSincePCreate)) as n, sum(sh) as sh from cohorts where sh < {} and sh >=0 group by 1,2,3),
b as (select cid, cohort,week, 
      sum(sh)  over (partition by cid, cohort order by week) as shct,
      sum(n)  over (partition by cid, cohort order by week) as nct
      from a)
select cid, cohort, week, shct,nct, shct/nct as shavg from b 
""".format(shCut[0]))

def getAverageForMeasure(varname, varcut):
    OO = spark.sql("""
with a as (select cid, cohort, week, sum({varname}) as x, sum(sh) as sh from cohorts where sh < {shcut} and {varname} < {xcut} group by 1,2,3),
b as (select cid, cohort,week, 
      sum(sh)  over (partition by cid, cohort order by week) as shct,
      sum(x)  over (partition by cid, cohort order by week) as xct
      from a)
    select cid, cohort, week, shct, xct, xct/({cc}+shct)  as xm from b 
    """.format(shcut = shCut[0],xcut = varcut,varname = varname, cc=C))
    return OO


intensdf   = getAverageForMeasure('ah',varcut=ahCut[0])
searchesdf = getAverageForMeasure('search',varcut=searchCut[0])
domdf      = getAverageForMeasure('domainsvisited',varcut=domCut[0])
uridf      = getAverageForMeasure('uris',varcut=uriCut[0])

tabdf      = getAverageForMeasure('tabsopened',varcut=tabCut[0])


#cmCut     = cut('cm')
#cpCut     = cut('cp')
#ccCut     = cut('cc')
tcCut     = cut('tc')
tcdf      = getAverageForMeasure('tc',varcut=tcCut[0])
#cpdf      = getAverageForMeasure('cp',varcut=cpCut[0])
#ccdf      = getAverageForMeasure('cc',varcut=ccCut[0])

## Saving the data
import subprocess
for i in ["newprofilesByCohort","activeProfiles","crashprofiles","shdf",'intensdf','searchesdf','domdf'
          ,'uridf','tabdf','tcdf']:
    print(i)
    O = "s3://mozilla-metrics/user/sguha/tmp/56vs57new/{}".format(i)
    subprocess.call(["aws", "s3", "rm", "--recursive", O])
    write = pyspark.sql.DataFrameWriter(globals()[i].coalesce(1))
    write.csv(path=O ,mode='overwrite',compression='none',sep=',',nullValue='NA',header=True)
        
```

