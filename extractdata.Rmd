# Extracting Data

## From main summary


We take  a 0.1% sample from all release Firefox profiles between 2016-08-01 till
2017-12-02. The data is grouped by client id, profile creation date , date of
activity and the version they were on.

```{python eval=FALSE}


import sys
import datetime
import random
import subprocess

from pyspark.sql import SparkSession
spark = SparkSession.builder.getOrCreate()
ms = spark.read.option("mergeSchema", "true").\
     parquet("s3://telemetry-parquet/main_summary/v4/")
ms.createOrReplaceTempView('ms')

st= """
SELECT
client_id as cid,
date_format(date_add('1970-01-01', profile_creation_date),'yyyy-MM-dd') as pcd,
substr(subsession_start_date,1,10) as date,
substr(app_version,1,2) as version,
sum(subsession_length / 60.0 / 60) AS sh,
sum(active_ticks * 5 /3600.0) as ah,
sum(coalesce(scalar_parent_browser_engagement_unique_domains_count,0))  as domainsvisited,
sum(coalesce(scalar_parent_browser_engagement_total_uri_count,0))  AS uris,
sum(coalesce(scalar_parent_browser_engagement_tab_open_event_count,0) +
coalesce(scalar_parent_browser_engagement_window_open_event_count,0)) as tabsopened,
sum(coalesce(crash_submit_success_main,0)) as cm,
sum(coalesce(crashes_detected_content, 0))- sum(coalesce(shutdown_kill,0)) as cc,
sum(coalesce(crashes_detected_plugin,0))+sum(coalesce(crashes_detected_gmplugin,0)) as cp
FROM ms
WHERE
normalized_channel = 'release'
and app_name = 'Firefox'
and substr(subsession_start_date,1,10) >= '{minsubsession}' and substr(subsession_start_date,1,10)<='{maxsubsession}'
and submission_date_s3 >= '{minsubsession}' and submission_date_s3 <= '{maxsubmission}'
and sample_id = '23'
and subsession_length>=0
and subsession_length<=86400
group by 1,2,3,4
""".format(
    minsubmission='20160801',
    maxsubmission='20171204',
    minsubsession='2016-08-01',
    maxsubsession='2017-12-04')
ut=spark.sql(st)
ut.createOrReplaceTempView ("ut")


spark.sql("select count(*), count(distinct(cid)) from ut").collect()

## Full blown parquet file
O = "s3://mozilla-metrics/user/sguha/tmp/snapshotForR"
subprocess.call(["aws", "s3", "rm", "--recursive", O])
write=pyspark.sql.DataFrameWriter(ut)
write.parquet("s3://mozilla-metrics/user/sguha/tmp/snapshotForR",mode='overwrite')

## Small CSV File
O = "s3://mozilla-metrics/user/sguha/tmp/snapshot05pct"
ut2=spark.sql(""" select * from ut where crc32(encode(cid, 'UTF-8')) % 1000 < 500""")
ut2.createOrReplaceTempView("ut2")
write = pyspark.sql.DataFrameWriter(ut2.coalesce(1))
write.csv(path=O ,mode='overwrite',compression='none',sep=',',nullValue='NA',header=True)

```


## Modifying in R 

```{r include=FALSE}
X=FALSE
C <- 1/60
FAC <- 500
```

I've found studying data using data tables and R is so much easier than creating
small data sets in spark and python. A 0.1% sample is more than sufficient for
these large scale analyses.


*Download the data from S3*, and do some simple modifications. 

```{r getdata0,eval=FALSE,cache=TRUE}
library(data.table)
system("rm -rf /tmp/x")
system(sprintf("aws s3 sync s3://mozilla-metrics/user/sguha/tmp/snapshot1pct/ /tmp/x"))
```

```{r getdata,eval=TRUE,cache=TRUE,dependson='getdata0'}
d0 <- fread(list.files("/tmp/x/",full=TRUE,pattern='*.csv'),
            colClasses=c("character","date","date","character",
                         "numeric","numeric","numeric","numeric",
                         "numeric","numeric","numeric","numeric"
                         ))
d0 <- d0[pcd>='2016-01-01' & date>='2016-01-01' & pcd <='2018-12-31' & date<='2018-12-31',]
d0 <- d0[!is.na(pcd) & !is.na(date),]
d0[, ":="(date=as.Date(date), pcd=as.Date(pcd),version=as.integer(version),crash=cm+cc, cr=(cm+cc)>0)]
d0[, cidf := as.integer(as.factor(cid))]
setkey(d0, cidf,date,pcd)
d0[, cid:=NULL]
```

## New Profiles

### New Profiles on a Version

*Create* a dataset with profiles that are *new* on a version and preserve the
first 3 weeks of them being on the new version. We will first get the versions
available to us. 

```{r prodversion, cache=TRUE}
library(rjson)
download.file("https://product-details.mozilla.org/1.0/firefox_history_major_releases.json","/tmp/rel.json")
versions <- fromJSON(file="/tmp/rel.json")
versions <- data.table(version=as.numeric(substr(names(versions),1,2)), date=as.Date(unlist(versions)))[version>=49,]
```

The rules for these profiles are

1. Their profile creation date is in the first seven days since version release
2. The version they are created on is the version in question
2. The proportion of profiles with data before PCD is very small (teensy like)
3. We keep the first 3 weeks of their data

Note, that these are not all new profiles on the time after release. New
profiles in that time period can come from other versions.

```{r newOnVersion,cache=TRUE,dependson='getdata'}
newonv <- lapply(versions$version,function(v){
    cat(sprintf("%s ",v))
    reldate <- versions[version==v, c(beg=date,end=date+6)]
    d1 <- d0[pcd %between% reldate,]
    check <- merge(d0,d1[,list(d=1),by=cidf],by='cidf')[date<(pcd-1),][, list(date,pcd) ,by=cidf]
    if((errpr <- length(unique(check$cidf))/length(unique(d1$cidf))*100)>0.5)
        stop(sprintf("You have many profiles with dates of activity before the profile creation date: %s",errpr))
    cat(sprintf(" bad data:%s\n",errpr))
    C <- 1/60
    d1 <- d1[date>=pcd,][, {
        if( length(version[date>=pcd])>0 && version[date>=pcd][1]==v ){
            list(pcd=pcd[1],version=v,release=reldate[1],
                 daysSince = as.numeric(date -pcd[1]),
                 sh=sh,
                 ah=ah,
                 dom = (domainsvisited),
                 uris = (uris),
                 tabs = (tabsopened),
                 crash =(crash),
                 cr = cr,
                 active = sh>0 ,
                 active2 = sh>0 & uris>0
                 )
        }
    },by=cidf][daysSince<=21,]
    d1
}) 
newonv <- rbindlist(newonv)
```

## Existing Users

For this data set, 

- for every version, 
- we keep profiles that updated to this version in one  weeks since release 
- have been on some version less than this version
- keep two weeks of prior use and two weeks of post use (they need not have been
  active and if so we will mark it as such)
  - these are labeled as first week after update
  - and 2nd week after update


```{r existData, cache=TRUE,eval=TRUE,dependson='getdata'}
exisd <- versions[,{
    templ <- data.table(when=c(0,1,2),ndays=0,sh=0,ah=0,uris=0,tabs=0,crash=0,active=0,cr=0)
    V <- .BY$version
    print(V)
    s1 <- .BY$date
    s2 <- s1+7
    upd <- d0[ sh<=24 & date>=s1 & date<=s2 & version==V,][, data.table(cidf=unique(cidf))]
    upd <- merge(d0,upd, by='cidf')
    upd <- merge(upd[, list(hasOlder =   any(version < V)),by=cidf][hasOlder==TRUE,list(cidf)],upd,by='cidf')
    upd[, dateOnV:=min(date[version==V]),by=cidf]
    upd[, postPart1:=dateOnV+ 7]
    upd[, postPart2:=dateOnV+ 14]
    upd[, preEnd:=dateOnV-14]
    p1 <- upd[date<dateOnV & date >=preEnd,][,when:=0]
    p2 <- upd[date>=dateOnV & date<postPart1,][,when:=1]
    p3 <- upd[date>=postPart1 & date<postPart2,][,when:=2]
    p=rbind(p1,p2,p3)
    setkey(p,cidf,pcd,date)
    pp <- p[, list(ndays=.N*1,sh=sum(sh),ah=sum(ah),uris=sum(uris),tabs=sum(tabsopened),crash=sum(crash),
                   active=1*(sum(sh)>0), cr=1*(sum(crash)>0))
           ,by=list(cidf,when)]
    pp[, {
        w0 <- when ==0;w1 <- when==1;w2 <- when==2; wall <- when==1 | when==2
        data.table(npre      =isn(ndays[w0],0),
                  shpre      = isn(sh[w0],0),
                  ahpre      = isn(ah[w0],0),
                  uripre     = isn(uris[w0],0),
                  tabpre     = isn(tabs[w0],0),
                  crashpre   = isn(crash[w0],0),
                  crpre      = isn(crash[w0],0)>0,
                  shpo1      = isn(sh[w1],0),
                  npo1       = isn(ndays[w1],0),
                  ahpo1      = isn(ah[w1],0),
                  uripo1     = isn(uris[w1],0),
                  tabpo1     = isn(tabs[w1],0),
                  crashpo1   = isn(crash[w1],0),
                  actpo1     = isn(sh[w1],0)>0,
                  crpo1      = isn(crash[w1],0)>0,
                  shpo2      = isn(sh[w2],0),
                  npo2       = isn(ndays[w2],0),
                  ahpo2      = isn(ah[w2],0),
                  uripo2     = isn(uris[w2],0),
                  tabpo2     = isn(tabs[w2],0),
                  crashpo2   = isn(crash[w2],0),
                  crpo2      = isn(crash[w2],0)>0,
                  actpo2     = isn(sh[w2],0)>0,
                  npoall     = isn(sum(ndays[wall]),0),
                  shpoall    = isn(sum(sh[wall]),0),
                  ahpoall    = isn(sum(ah[wall]),0),
                  uripoall   = isn(sum(uris[wall]),0),
                  tabpoall   = isn(sum(tabs[wall]),0),
                  crashpoall = isn(sum(crash[wall]),0),
                  crpoall    = isn(sum(crash[wall]),0)>0,
                  actpoall   = isn(sum(sh[wall]),0)>0)
    }, by=cidf]
},by=list(date,version)]

```


```{r saveData,cache=TRUE,dependsOn='newOn'}
save(d0, newonv,versions,exisd,FAC, file="~/tmp/snapshot1pct.Rdata")
```
