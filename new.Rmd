```{r dataImport,eval=FALSE}
cohortSize <- fread("/tmp/betanew_cohort.csv")
activityByWeek <- fread("/tmp/betanew_activen.csv")
```



```{r bootRel,eval=FALSE}
bootActivity <- function(d0,i,wk,pb){
    d <- d0[i,]
    a <- d[week==wk,][, length(unique(cid)),by=cohort]
}
```
    
### Data Extraction
#### Python Extraction Code


We know that Firefox Release 56 was released on August 8, 2017 and Firefox Beta 57
was released on September 25, 2017.

What is a random sample of **new release 56 users**? Anybody who downloaded version 56
between August 8, 2017 and September 24, 2017. The days they were on beta 56 would
correspond to their release 56 usage. Similarly for 57. We restrict out download
days to the time when 56 was the most recent version available as opposed to
including new users of 56 when 57 was available. 

However to get results out in a timely manner, we'll focus on new profiles
created in first two weeks of release of the corresponding beta and two weeks of
data thereafter.



```{python eval=FALSE}
import sys
import datetime
import random
import subprocess
import mozillametricstools.common.functions as mozfun

from pyspark.sql import SparkSession
spark = SparkSession.builder.getOrCreate()
ms = spark.read.option("mergeSchema", "true").\
     parquet("s3://telemetry-parquet/main_summary/v4/")
ms.createOrReplaceTempView('ms')

import mozillametricstools.common.functions as mozfun
mozfun.register_udf(sqlContext
                    , lambda arr:  sum(arr) if arr else 0, "array_sum"
                    , pyspark.sql.types.IntegerType())

def newprofiles(version,minpcd,maxpcd,minsubmission,maxsubmission):
    newprofiles = spark.sql(
        """
        SELECT
        client_id as cid,
        'new' as persona,
        submission_date_s3 as date,
        profile_creation_date as pcd,
        datediff(
        from_unixtime(unix_timestamp(submission_date_s3, 'yyyyMMdd')),
        date_add('1970-01-01', profile_creation_date)
        ) AS tSincePCreate,
        '{version}' as cohort,
        count(*) as ns,
        sum(subsession_length / 60.0 / 60) AS sh,
        sum(active_ticks * 5 /3600.0) as ah,
        sum(case 
            when search_counts is not null then array_sum(search_counts.count) else 0 
        end) as searches,
        sum(case when  scalar_parent_browser_engagement_unique_domains_count is null 
            then  0 else  scalar_parent_browser_engagement_unique_domains_count end ) as domainsvisited,
        sum(case when  scalar_parent_browser_engagement_total_uri_count is null then 0 else 
            scalar_parent_browser_engagement_total_uri_count end) AS uris,
        sum(case when  scalar_parent_browser_engagement_tab_open_event_count is null 
            then 0 else  scalar_parent_browser_engagement_tab_open_event_count end) as tabsopened,
        sum(case when crash_submit_success_main is null then 0 else crash_submit_success_main end) as cm,
        sum(case when crashes_detected_content is null then 0 else crashes_detected_content end)  -
             sum(case when shutdown_kill is null then 0 else shutdown_kill  end) as cc,
        sum(case when crashes_detected_plugin  is null then 0 else crashes_detected_plugin end) +   
            sum(case when crashes_detected_gmplugin  is null then 0 else crashes_detected_gmplugin end) as cp
        FROM ms
        WHERE
        normalized_channel = 'release'
        and app_name = 'Firefox'
        and app_version = '{version}'
        and profile_creation_date >= {minpcd} and profile_creation_date<={maxpcd} 
        and submission_date_s3 >= '{minsubmission}' and submission_date_s3 <= '{maxsubmission}'
        group by 1,2,3,4,5,6
        """.format(version=version,
                   minpcd=minpcd,
                   maxpcd=maxpcd, 
                   minsubmission=minsubmission,
                   maxsubmission=maxsubmission)
    )
    newprofiles.createOrReplaceTempView("np")
    f = spark.sql(""" select *,
case when cm+cc+cp >0  then 1 else 0 end as crq,
case when sh>0 and uris>0 then 1 else 0 end as active,
case when  tSincePCreate  <=6 then 1 
            when  tSincePCreate  <=13 then 2 
            when  tSincePCreate <=27 then 3
            when  tSincePCreate <=34 then 4
            else 5 end as week
from np where pcd>0 and sh>=0""")
    return f

## Get dates for 56
releases = { 
    '55': datetime.datetime.strptime("2017-08-08","%Y-%m-%d").date(),
    '56': datetime.datetime.strptime("2017-09-25","%Y-%m-%d").date(),
    '57': datetime.datetime.strptime("2017-11-15","%Y-%m-%d").date(),
    'origin': datetime.datetime.strptime("1970-01-01","%Y-%m-%d").date()
}
cohort56 = newprofiles('55.0', 
                       minpcd=(releases['56']-releases['origin']).days,
                       maxpcd=(releases['56']-releases['origin']).days+14,
                       minsubmission=releases['56'].strftime("%Y%m%d"),
                       maxsubmission=(releases['56']+datetime.timedelta(days=28-1)).strftime("%Y%m%d"))
cohort57 = newprofiles('56.0', 
                       minpcd=(releases['57']-releases['origin']).days,
                       maxpcd=(releases['57']-releases['origin']).days+14,
                       minsubmission=releases['57'].strftime("%Y%m%d"),
                       maxsubmission=(releases['57']+datetime.timedelta(days=28-1)).strftime("%Y%m%d"))
cohorts = cohort57.union(cohort56)
cohorts = cohorts.cache()
cohorts.createOrReplaceTempView("cohorts")

## Saving the data
import subprocess
O = "s3://mozilla-metrics/user/sguha/tmp/betanew"
subprocess.call(["aws", "s3", "rm", "--recursive", O])
write = pyspark.sql.DataFrameWriter(cohorts)
write.parquet(path=O ,mode='overwrite')

cohorts  = spark.read.option("mergeSchema", "true").parquet(O)
cohorts.createOrReplaceTempView("cohorts")
cohorts = cohorts.cache()

```

##### 1.Create record of new profiles in each cohort

```{python eval=FALSE}
newprofilesByCohort = spark.sql(
    """ select cohort, 
    count(distinct(cid)) as n 
    from cohorts 
    group by cohort 
    order by 1""")
```


##### 2.Create a database of client_id, cohort ...
and whether profile was active in week 1 or week 2 (not cumulative). This and
(1) are enough for the retention bit
 
```{python eval=FALSE}
activeProfiles = spark.sql(""" select cid, cohort,
 week, case when sum(active)>0 then 1 else 0 end as activeIn from cohorts group
 by 1,2,3 """)
```

##### 3.For measures such session length, active session length, ...
intensity, domains, uris, tabsopenend, we need to look at the average value per
profile cumulative till week i. However,

- filter out rows for which measure is greater than the 99.9% (for that measure)
- add `C = 1/60` to the denominator for things like domain/per hour/profile (since we often get NaN)

```{python eval=FALSE}
pcut = [0.999]
def cut(v):
    return cohorts.approxQuantile(v,pcut,0.01)

C = 1/60.0
shCut = cut('sh')
ahCut =cut('ah')
searchCut = cut('searches')
domCut = cut('domainsvisited')
uriCut = cut('uris')
tabCut = cut('tabsopened')

shdf = spark.sql("""
with a as (select cid, cohort, week, count(*) as n, sum(sh) as sh from cohorts where sh < {} group by 1,2,3),
b as (select cid, cohort,week, 
      sum(sh)  over (partition by cid, cohort order by week) as shct,
      sum(n)  over (partition by cid, cohort order by week) as nct
      from a)
select cid, cohort, week, shct/nct as shavg from b 
""".format(shCut[0]))

def getAverageForMeasure(varname, varcut):
    OO = spark.sql("""
with a as (select cid, cohort, week, sum({varname}) as x, sum(sh) as sh from cohorts where sh < {shcut} and {varname} < {xcut} group by 1,2,3),
b as (select cid, cohort,week, 
      sum(sh)  over (partition by cid, cohort order by week) as shct,
      sum(x)  over (partition by cid, cohort order by week) as xct
      from a)
    select cid, cohort, week, shct, xct, xct/({cc}+shct)  as xm from b 
    """.format(shcut = shCut[0],xcut = varcut,varname = varname, cc=C))
    return OO


intensdf   = getAverageForMeasure('ah',varcut=ahCut[0])
searchesdf = getAverageForMeasure('searches',varcut=ahCut[0])
domdf      = getAverageForMeasure('domainsvisited',varcut=ahCut[0])
uridf      = getAverageForMeasure('uris',varcut=ahCut[0])
tabdf      = getAverageForMeasure('tabsopened',varcut=ahCut[0])


cmCut     = cut('cm')
cpCut     = cut('cp')
ccCut     = cut('cc')
cmdf      = getAverageForMeasure('cm',varcut=cmCut[0])
cpdf      = getAverageForMeasure('cp',varcut=cpCut[0])
ccdf      = getAverageForMeasure('cc',varcut=ccCut[0])

## Saving the data
import subprocess
for i in ["newprofilesByCohort","activeProfiles","shdf",'intensdf','searchesdf','domdf','uridf','tabdf','cmdf','cpdf','ccdf']:
    print(i)
    O = "s3://mozilla-metrics/user/sguha/tmp/56vs57new/{}".format(i)
    subprocess.call(["aws", "s3", "rm", "--recursive", O])
    write = pyspark.sql.DataFrameWriter(globals()[i].coalesce(1))
    write.csv(path=O ,mode='overwrite',compression='none',sep=',',nullValue='NA',header=True)
        
```
